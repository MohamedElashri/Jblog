<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="apple-mobile-web-app-capable" content="yes"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title> MiniBooNE Particle Identification | My Blog </title> <meta name="description" content=" In this blog post, I&#39;m going to walk through about applying machine learning algorithms in particle physics. There are many available datasets ready for ML b... "> <meta name="keywords" content="MiniBooNE, Particle, physics, particle-identification, Fermilab, ML, Machine Learning"> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <!-- Social: Twitter --> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:site" content="MohamedElashri"> <meta name="twitter:title" content=" MiniBooNE Particle Identification | My Blog "> <meta name="twitter:description" content=" In this blog post, I&#39;m going to walk through about applying machine learning algorithms in particle physics. There are many available datasets ready for ML b... "> <meta name="twitter:image:src" content="https://blog.melashri.net"> <!-- Canonical link tag --> <link rel="canonical" href="https://blog.melashri.net/2020/MiniBooNE-particle-identification/"> <link rel="alternate" type="application/rss+xml" title="My Blog" href="https://blog.melashri.net/feed.xml"> <!-- rel prev and next --> <link rel="stylesheet" href="https://blog.melashri.net/assets/css/main.css"> <link rel="stylesheet" href="/assets/css/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> </head> <body> <main class="wrapper"> <header class="site-header"> <nav class="nav"> <div class="container"> <h1 class="logo"><a href="https://blog.melashri.net/">My Blog<span></span></a> </h1> <ul class="navbar"> <li><a href="https://blog.melashri.net/about">About</a></li> <li><a href="https://blog.melashri.net/feed.xml" target="_blank">RSS</a></li> </ul> </div> </nav> </header> <article class="post container" itemscope itemtype="http://schema.org/BlogPosting"> <header class="post-header"> <p class="post-meta"><time datetime="2020-08-20T00:00:00+00:00" itemprop="datePublished">Aug 20, 2020</time></p> <h1 class="post-title" itemprop="name headline">MiniBooNE Particle Identification</h1> </header> <div class="post-content" itemprop="articleBody"> <hr /> <ul> <li><a href="#introduction">Introduction</a> <ul> <li><a href="#data">Data</a></li> <li><a href="#evaluation">Evaluation</a> <ul> <li><a href="#definitions">Definitions</a></li> </ul> </li> <li><a href="#models">Models</a> <ul> <li><a href="#logistic-regression">Logistic regression</a></li> <li><a href="#k-nearest-neighbors">K-nearest neighbors</a></li> <li><a href="#decision-trees">Decision Trees</a></li> <li><a href="#random-forest">Random Forest</a></li> </ul> </li> <li><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> <h1 id="introduction">Introduction</h1> <p>In this blog post, I’m going to walk through about applying machine learning algorithms in particle physics. There are many available datasets ready for ML but one of a particular interest is the MiniBooNE dataset available on <a href="https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification">UCI machine learning repository</a>. This dataset is used for a published <a href="https://inspirehep.net/literature/657972">paper</a> from the experiment collaboration which is very interesting to read although it is somehow old. Particle Identification was part of the early success stories for machine learning and deep learning so it was applied since early 2000’s.</p> <h2 id="data">Data</h2> <ul> <li>The dataset has been taken from UCI machine learning repository</li> <li>It has been taken from the MiniBooNE experiment conducted in the fermilab.</li> <li>A stream of muon neutrinos are fired and the detector measures the precense of electron neutrinos(signal) among the muon neutrinos(noise).</li> <li>There are 50 features in the dataset related to every detection made , however no information is given about the features.</li> <li>There are no missing values.</li> <li>The first line in the file MiniBooNE_PID.txt contains 2 space seperated values , the signal events come first, followed by the background events</li> <li>This is a binary classification problem where we want to tell wether a given signal is a electron neutrino or not.</li> </ul> <p>we can download the dataset directly in our python/jupyter workspace using:</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget -O data.txt -nc --no-check-certificate  https://archive.ics.uci.edu/ml/machine-learning-databases/00199/MiniBooNE_PID.txt
</code></pre></div></div> <p>we will need to do some data processing to make the data ready for more steps. we are going to do the following:</p> <ul> <li>The data is stored in <code class="highlighter-rouge">data.txt</code> which was downloaded from the above link.</li> <li>We use the pandas library to read the data and skip the first row as it contains the number of positive and negative labels</li> <li>Create a numpy array of 1’s(electron neutrino) and 0’s(muon neutrino) which acts as our labels for the classification problem.</li> <li>Convert the input dataframe into a numpy array for the analysis.</li> <li>After having a look at the data we see that there are many features having large values. This makes the Machine Learning algorithms difficult to converge to a result. Therefore, the solution is to scale down the data.</li> <li>We just want the range to change, not the mean, or the variance so that the data still caries the information it did before scaling. Hence a good scaler to use is the minmax scaler.</li> <li>Use the train test split to split the data into training and test data sets with a default of 75% training data and 25% test data.</li> </ul> <p>Lets code this</p> <p>We need to import the needed packages</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div> <p>define data and read it into pandas dataframe</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data.txt'</span><span class="p">,</span><span class="n">sep</span><span class="o">=</span><span class="s">' '</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">skipinitialspace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>We explore data features</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0	1	2	3	4	5	6	7	8	9	...	40	41	42	43	44	45	46	47	48	49
0	2.59413	0.468803	20.6916	0.322648	0.009682	0.374393	0.803479	0.896592	3.59665	0.249282	...	101.174	-31.3730	0.442259	5.86453	0.000000	0.090519	0.176909	0.457585	0.071769	0.245996
1	3.86388	0.645781	18.1375	0.233529	0.030733	0.361239	1.069740	0.878714	3.59243	0.200793	...	186.516	45.9597	-0.478507	6.11126	0.001182	0.091800	-0.465572	0.935523	0.333613	0.230621
2	3.38584	1.197140	36.0807	0.200866	0.017341	0.260841	1.108950	0.884405	3.43159	0.177167	...	129.931	-11.5608	-0.297008	8.27204	0.003854	0.141721	-0.210559	1.013450	0.255512	0.180901
3	4.28524	0.510155	674.2010	0.281923	0.009174	0.000000	0.998822	0.823390	3.16382	0.171678	...	163.978	-18.4586	0.453886	2.48112	0.000000	0.180938	0.407968	4.341270	0.473081	0.258990
4	5.93662	0.832993	59.8796	0.232853	0.025066	0.233556	1.370040	0.787424	3.66546	0.174862	...	229.555	42.9600	-0.975752	2.66109	0.000000	0.170836	-0.814403	4.679490	1.924990	0.253893
</code></pre></div></div> <p>Now we need to scale the data</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">file</span><span class="o">=</span><span class="nb">open</span><span class="p">(</span><span class="s">'data.txt'</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="nb">file</span><span class="p">.</span><span class="n">readline</span><span class="p">()</span>
<span class="nb">file</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">numlabels</span><span class="o">=</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">y</span><span class="p">.</span><span class="n">split</span><span class="p">()]</span>
<span class="n">ylabels</span><span class="o">=</span><span class="n">numlabels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">numlabels</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">ylabels</span><span class="p">)</span>
<span class="n">X</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">=</span><span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_scaled</span><span class="o">=</span><span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div> <p>Then split the dataset into training and test sets</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="o">=</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

</code></pre></div></div> <h2 id="evaluation">Evaluation</h2> <p>we are going to apply different ML algorithms on our data and will need to determine a rubric to evaluate our models. We define a function for model evaluation based on the confusion matrix. The confusion matrix is used to quantify how many of the predicted values were correct and incorrect.</p> <h3 id="definitions">Definitions</h3> <p><strong>Accuracy</strong>: The number of true predictions(true 0’s and true 1’s) divided by the total number of predictions made <strong>Precicion</strong>: The number of true 1’s divided by the total number of 1’s predicted.(Basically telling us that how well have we predicted the 1’s) precision=1 if no 1’s are predicted as 0 (precision=TP/(TP+FP)) <strong>Recall</strong>: The number of true 1’s divided by the actual 1’s.(the fraction of correctly classified 1’s) . recall=1 if no 1s are predicted as 0.(recall=TP/(TP+FN)) <strong>ROC</strong>: a graph where false positive rate is plotted on the X-axis and true positive rate is plotted in the Y axis. The area under the ROC curve is a good measure of how well the algorothm has performed. A score close to 1 is a good auc(area under the curve) score.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_pred_proba</span><span class="p">):</span>
    <span class="n">cnf_matrix</span><span class="o">=</span><span class="n">metrics</span><span class="p">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'The confusion matrix for the given model is: '</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">cnf_matrix</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'accuracy : '</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'precision : '</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'recall : '</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>  <span class="n">y_pred_proba</span><span class="p">)</span>
    <span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="n">tpr</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">'Area under the curve= '</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">auc</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'ROC curve'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False positive rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive rate'</span><span class="p">)</span>
</code></pre></div></div> <h2 id="models">Models</h2> <p>In this analysis we will try different machine learning algorithm and seek the best model among them. We will use the following models</p> <ol> <li>Logistic regression</li> <li>K-nearest Neigbhors</li> <li>Decision trees</li> <li>SVM (Support Vector Machines)</li> <li>Random Forest</li> </ol> <h3 id="logistic-regression">Logistic regression</h3> <p>Logistic regression uses the sigmoid function to estimate the probability of an instance being classified as 1. The C value controls large values for weights that may lead to over fitting in the data</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># define the model
</span><span class="n">lr</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">C_range</span><span class="o">=</span><span class="p">{</span><span class="s">'C'</span><span class="p">:[</span><span class="mi">100</span><span class="p">]}</span>
<span class="n">clf</span><span class="o">=</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span><span class="n">C_range</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="c1"># print model scores
</span><span class="k">print</span><span class="p">(</span><span class="s">'The score for this model is: '</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'the best value of parameter C is: '</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_proba</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[::,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># evaluate the model
</span><span class="n">evaluate</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_pred_proba</span><span class="p">)</span>
</code></pre></div></div> <p>The output will be the following</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The score <span class="k">for </span>this model is:  0.8730778693566245 
the best value of parameter C is:  <span class="o">{</span><span class="s1">'C'</span>: 100<span class="o">}</span> 
The confusion matrix <span class="k">for </span>the given model is: <span class="o">[[</span>22184  1268] <span class="o">[</span> 2859  6205]] accuracy :  0.8730778693566245 
precision :  0.8303224943128596 
recall :  0.684576345984113
</code></pre></div></div> <p><img src="/assets/images/posts/MinniBooNE/2.png" alt="Logistic regression output" class="center-image" /></p> <h3 id="k-nearest-neighbors">K-nearest neighbors</h3> <p>The K-nearest neighbors model does not actually train a model based on the data but rather stores all the training data given to it and then calculates the distance of each point from every other point.When test data is given, it classifies it as a 1 or 0 based on votes based on the chosen k(number of nearest neighbors). It is unsupervised learning algorithm</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="c1"># define the model
</span><span class="n">knn</span><span class="o">=</span><span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">parameters_knn</span><span class="o">=</span><span class="p">{</span><span class="s">'n_neighbors'</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">14</span><span class="p">]}</span>
<span class="n">clf</span><span class="o">=</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span><span class="n">parameters_knn</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print model scores
</span><span class="k">print</span><span class="p">(</span><span class="s">'The score for this model is: '</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'the best value of parameters is: '</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_proba</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[::,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># evaluate the model
</span><span class="n">evaluate</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_pred_proba</span><span class="p">)</span>
</code></pre></div></div> <p>The output will be the following</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The score <span class="k">for </span>this model is:  0.8901463894697995
the best value of parameters is:  <span class="o">{</span><span class="s1">'n_neighbors'</span>: 14<span class="o">}</span>
The confusion matrix <span class="k">for </span>the given model is: <span class="o">[[</span>21821  1631] <span class="o">[</span> 1941  7123]] 
accuracy :  0.8901463894697995 
precision :  0.8136851724925748
recall :  0.785856134157105
</code></pre></div></div> <p><img src="/assets/images/posts/MinniBooNE/3.png" alt="Logistic regression output" class="center-image" /></p> <h3 id="decision-trees">Decision Trees</h3> <p>A Binary Decision Tree is a structure based on a sequential decision process. Starting from the root, a feature is evaluated and one of the two branches is selected. This procedure is repeated until a final leaf is reached, which normally represents the classification target we are looking for. The model can over fit if no limit is specified on the depth the tree can go to.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="c1"># define the model
</span><span class="n">dt</span><span class="o">=</span><span class="n">tree</span><span class="p">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">parameters_dt</span><span class="o">=</span><span class="p">{</span><span class="s">'max_depth'</span><span class="p">:[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">]}</span>
<span class="n">clf</span><span class="o">=</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span><span class="n">parameters_dt</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print model scores
</span><span class="k">print</span><span class="p">(</span><span class="s">'The score for this model is: '</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'the best value of parameters is: '</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_proba</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[::,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># evaluate the model
</span><span class="n">evaluate</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_pred_proba</span><span class="p">)</span>
</code></pre></div></div> <p>The output will be the following</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The score <span class="k">for </span>this model is:  0.908721860007381 
the best value of parameters is:  <span class="o">{</span><span class="s1">'max_depth'</span>: 10<span class="o">}</span> 
The confusion matrix <span class="k">for </span>the given model is: <span class="o">[[</span>21899  1553] <span class="o">[</span> 1415  7649]] 
accuracy :  0.908721860007381
precision :  0.8312323407954793 
recall :  0.8438879082082965
</code></pre></div></div> <p><img src="/assets/images/posts/MinniBooNE/4.png" alt="Logistic regression output" class="center-image" /></p> <h3 id="random-forest">Random Forest</h3> <p>random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># define the model
</span><span class="n">rf</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">bootstrap</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">parameters_rf</span><span class="o">=</span><span class="p">{</span><span class="s">'n_estimators'</span><span class="p">:[</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span><span class="s">'max_depth'</span><span class="p">:[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span><span class="s">'max_samples'</span><span class="p">:[</span><span class="mi">30000</span><span class="p">,</span><span class="mi">40000</span><span class="p">]}</span>
<span class="n">clf</span><span class="o">=</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span><span class="n">parameters_rf</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print model scores
</span><span class="k">print</span><span class="p">(</span><span class="s">'The score for this model is: '</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'the best value of parameters is: '</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_proba</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[::,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># evaluate the model
</span><span class="n">evaluate</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_pred_proba</span><span class="p">)</span>
</code></pre></div></div> <p>The output will be the following</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The score <span class="k">for </span>this model is:  0.925544347398204 
the best value of parameters is:  <span class="o">{</span><span class="s1">'max_depth'</span>: 10, <span class="s1">'max_samples'</span>: 40000, <span class="s1">'n_estimators'</span>: 100<span class="o">}</span> 
The confusion matrix <span class="k">for </span>the given model is: <span class="o">[[</span>22346  1106] <span class="o">[</span> 1315  7749]] 
accuracy :  0.925544347398204 
precision :  0.8750988142292491 
recall :  0.8549205648720212

</code></pre></div></div> <p><img src="/assets/images/posts/MinniBooNE/5.png" alt="Logistic regression output" class="center-image" /></p> <h2 id="conclusion">Conclusion</h2> <p>In our Analysis we find “Random forest” is the best algorithm with the highest ROC value</p> </div> </article> <aside id="comments" class="isso"> <div class="container"> <h3><i class="icon icon-comments-o"></i> Comments</h3> <div id="isso_thread"></div> <script data-isso="https://comments.melashri.eu.org/" data-isso-avatar="true" data-isso-vote="true" data-isso-vote-levels="-5,5" src="https://comments.melashri.eu.org/js/embed.min.js"></script> <section id="isso-thread"></section> <noscript> Please enable JavaScript to view the <a href="https://blog.melashri.net" rel="nofollow">comments powered by ISSO.</a> </noscript> </div> </aside> <footer class="site-footer"> <div class="container"> <small class="block">&copy; 2023 Mohamed Elashri &middot; &lt;/&gt; Powered by <a href="https://jekyllrb.com/">Jekyll</a> and <a href="https://github.com/VerifiedGruber/thinkspace">Thinkspace theme</a></small> </div> </footer> </main> </body> </html>
